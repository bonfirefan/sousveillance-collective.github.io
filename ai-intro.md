# Introduction to large-scale surveillance systems and how they work

In Guiyang, the police found a BBC reporter using the city's facial-recognition-powered surveillance system in seven minutes. (See BBC video: [In your face: China's all-seeing state](https://www.bbc.com/news/av/world-asia-china-42248056/in-your-face-china-s-all-seeing-state)) What technical, social, and legal infrastructure made this achievement possible? And what are the civil liberties issues with pervasive surveillance?

First, physical infrastructure manifests itself as the familiar "eyes of big brother": surveillance cameras. Behind the scenes, IP cameras rely on internet infrastructure. 

Second, everything that can be turned into data will be turned into data: for example, Google Street View, the government's "multiple encounter deceased" dataset for facial recognition, and scraping millions of Flickr photos online. "If you’re an adult in America, there’s more than a 50 percent chance that you’re already in a law enforcement facial recognition database, according to researchers at Georgetown." (Source: NYT [Facial Recognition Machine](https://www.nytimes.com/interactive/2019/04/16/opinion/facial-recognition-new-york-city.html))

Third, large-scale AI works by humans encoding community knowledge and norms. When you do a captcha or submit a Google form, you are helping train, say, a self-driving car to recognize what's a car and what's a road sign. Dataset labeling factories are becoming increasingly common (again, Guiyang, China is the frontier). (See _Sixth Tone_ article and video: [Tilling the data farms of Guizhou](http://www.sixthtone.com/news/1002222/tilling-the-data-farms-of-guizhou)) These dataset labeling factories use people for their "minimum viable humanity" to extract and capture their value for the profit of the very few.

What happens after dataset labeling? Since humans label data, it encodes community knowledge and biases. For example, in facial recognition, datasets are labeled with one of few emotions. Modern models learn to map an input (a picture of a face) to an output (an emotion label) by a process of learning from their mistakes over and over on a dataset spanning millions or billions of examples. Then a trained emotion detection model might be deployed in, say, a surveillance camera to recognize anomalies in public space, for example if a person looks fearful or angry, it might make a decision to call police. Again, note that your ML algorithm is limited by your dataset! The result is that old instruments for looking and capture become lenses augmented by massive data and compute to encode certain ways of looking, and to enforce these norms on public space, like Esther Hovers' series "False Positives.

Who's being looked at? People and their faces and bodies are constantly being analyzed and dissected by algorithms. Your faceprint is everywhere. Facebook image labeling, unlocking your phone.

Who's looking? Other people. Often not the people you expect. Voyeurs in the NYPD; your neighbors; hackers in another country; and you. The gaze becomes a part of you, something you carry around inside you.

Lastly: follow the money! Surveillance is big business. China's biggest facial recognition startups, Dahua and Hikvision, are worth billions.

Now that we've dissected how the police were able to find a reporter in Guiyang in just seven minutes, let's look at how people are looking back. What can we, as individuals, do against these large institutional forces?

First, surveillance technologies are generally made by members of more privileged communities and targeted at members of more marginalized communities. For example, take how ICE targets immigrants, or how NYPD targets the city's Muslim community, as two examples. This page gives a good overview of [civil liberties issues](http://theyarewatching.org/) with pervasive surveillance. There is basically no good surveillance.

Activists have proposed the idea of sousveillance to reverse the gaze, notably Simone Browne's "Dark Matters." Artists and academica have made attempts at mapping surveillance networks, controlling them with legislation or agreements, jamming them with individual actions. 

The next exercise will help us understand very concretely how people make datasets for ML models. It will also help us practice a critical way of seeing public space, work together to encode community knowledge, and help us hold surveillance technologies accountable.

Discussion questions: coming soon.
